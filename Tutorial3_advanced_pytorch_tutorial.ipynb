{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "advanced_pytorch_tutorial_1",
      "provenance": [],
      "private_outputs": true,
      "collapsed_sections": [],
      "toc_visible": true,
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/ajfisch/deeplearning_bootcamp_2020/blob/master/advanced_pytorch_tutorial.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GA8LLAO9dchw",
        "colab_type": "text"
      },
      "source": [
        "# Introduction to RNNs and CNNs in PyTorch\n",
        "In this tutorial, we'll take you through developing recurrent neural networks (RNNs) and convolutional neural networks (CNNs) in PyTorch to classify beer reviews.\n",
        "\n",
        "Let's get started!"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kcCj4gBWdFeI",
        "colab_type": "text"
      },
      "source": [
        "# Preliminaries\n",
        "\n",
        "The next few sections will set up the necessary components of the tutorial, including:\n",
        "\n",
        "\n",
        "1.   Installing PyTorch\n",
        "2.   Importing dependencies\n",
        "3.   Downloading and processing data\n",
        "4.   Defining training and evaluation procedures\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pg2T9UsXctbP",
        "colab_type": "text"
      },
      "source": [
        "## Download PyTorch"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eT_jLzv8do9z",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# http://pytorch.org/\n",
        "from os import path\n",
        "from wheel.pep425tags import get_abbr_impl, get_impl_ver, get_abi_tag\n",
        "\n",
        "platform = '{}{}-{}'.format(get_abbr_impl(), get_impl_ver(), get_abi_tag())\n",
        "accelerator = 'cu100' if path.exists('/opt/bin/nvidia-smi') else 'cpu'\n",
        "\n",
        "print(f'Platform = {platform}, Accelerator = {accelerator}')\n",
        "\n",
        "!pip install -q http://download.pytorch.org/whl/{accelerator}/torch-1.1.0-{platform}-linux_x86_64.whl\n",
        "!pip install -q torchvision==0.2.0\n",
        "\n",
        "import torch\n",
        "print(f'Torch version = {torch.__version__}')\n",
        "print(f'Cuda available = {torch.cuda.is_available()}')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "h7uKFRTBczh9",
        "colab_type": "text"
      },
      "source": [
        "## Imports"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mcLJrU16dch0",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import argparse\n",
        "from collections import Counter\n",
        "import pickle\n",
        "import re\n",
        "\n",
        "import numpy as np\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import DataLoader, Dataset\n",
        "from tqdm import tqdm"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "v7mgPaxPc1vx",
        "colab_type": "text"
      },
      "source": [
        "## Download and Process Data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DMJfz2f-JC9E",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "!apt-get install wget\n",
        "!wget https://raw.githubusercontent.com/yala/MLCodeLab/master/lab1/data/beer/overall_train.p\n",
        "!wget https://raw.githubusercontent.com/yala/MLCodeLab/master/lab1/data/beer/overall_dev.p\n",
        "!wget https://raw.githubusercontent.com/yala/MLCodeLab/master/lab1/data/beer/overall_test.p\n",
        "\n",
        "train_path = \"overall_train.p\"\n",
        "dev_path   = \"overall_dev.p\"\n",
        "test_path  = \"overall_test.p\"\n",
        "\n",
        "train_set =  pickle.load(open(train_path, 'rb'))\n",
        "dev_set =  pickle.load(open(dev_path, 'rb'))\n",
        "test_set =  pickle.load(open(test_path, 'rb'))\n",
        "\n",
        "def preprocess_data(data):\n",
        "    for indx, sample in enumerate(data):\n",
        "        text, label = sample['text'], sample['y']\n",
        "        text = re.sub('\\W+', ' ', text).lower().strip()\n",
        "        data[indx] = text, label\n",
        "    return data\n",
        "\n",
        "train_set = preprocess_data(train_set)\n",
        "dev_set = preprocess_data(dev_set)\n",
        "test_set =  preprocess_data(test_set)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "q_aWa4S8V8LM",
        "colab_type": "text"
      },
      "source": [
        "### Question:\n",
        "1. Why would we want to do this kind of preprocessing?\n",
        "\n",
        "2. In what kind of datasets would this kind of preprocessing be a bad idea?\n",
        "\n",
        "3. Before we go to working on the model, what questions should we ask about the data?\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rqdbCTTwV9fp",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "print(f'Num Train = {len(train_set):,}')\n",
        "print(f'Num Dev   = {len(dev_set):,}')\n",
        "print(f'Num Test  = {len(test_set):,}')\n",
        "print()\n",
        "\n",
        "trainText = [t[0] for t in train_set]\n",
        "trainY = [t[1] for t in train_set]\n",
        "\n",
        "devText = [t[0] for t in dev_set]\n",
        "devY = [t[1] for t in dev_set]\n",
        "\n",
        "testText = [t[0] for t in test_set]\n",
        "testY = [t[1] for t in test_set]\n",
        "\n",
        "allText = trainText + devText + testText\n",
        "\n",
        "print('Train class balance')\n",
        "y_count = Counter(trainY)\n",
        "for y in sorted(y_count.keys()):\n",
        "    print(f'{y} = {100. * y_count[y] / len(trainY):.2f}%')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "a_nrd1cJc_JM",
        "colab_type": "text"
      },
      "source": [
        "## Dataset Class\n",
        "\n",
        "### Review Question:\n",
        "1. What is the benefit of using a Pytorch Dataset Object?"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4smEQXOddciA",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class BeerReviewDataset(Dataset):\n",
        "    def __init__(self, X, Y):\n",
        "      self.X, self.Y = X, Y\n",
        "      assert len(X) == len(Y)\n",
        "\n",
        "    def __len__(self):\n",
        "       return len(self.X)\n",
        "\n",
        "    def __getitem__(self, i):\n",
        "      return np.array(self.X[i]), self.Y[i]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "M9hvStGzdcjC",
        "colab_type": "text"
      },
      "source": [
        "## Model and Training Settings\n",
        "\n",
        "### Review Questions:\n",
        "1. What does each of these variables mean?\n",
        "2. What is what might we expect if we use a batch size that is too small? too large?\n",
        "3. What is what might we expect if we use a learning rate that is too small? too large?\n",
        "3. What is what might we expect if we use a weight decay that is too small? too large?\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "n3Ht1mhhdcjE",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "batch_size = 16\n",
        "epochs = 20\n",
        "lr = 1e-3\n",
        "weight_decay = 1e-3\n",
        "max_len = 150\n",
        "embedding_size = 100\n",
        "hidden_size = 100\n",
        "output_size = 3\n",
        "dropout = 0.4\n",
        "use_cuda = True"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ztOopQZVPKgB",
        "colab_type": "text"
      },
      "source": [
        "## Utility Functions"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kQ2gLx_CPKls",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def param_count(model):\n",
        "    return sum(param.numel() for param in model.parameters() if param.requires_grad)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Gz35WFTOPDFo",
        "colab_type": "text"
      },
      "source": [
        "## Training Procedure\n",
        "\n",
        "### Review Questions:\n",
        "1. What are the steps in defining a training loop?\n",
        "2. What would happen if we removed `optimizer.zero_grad()`?\n",
        "3. Why do we detach the gradient from loss?"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wxIj0eWsdcjJ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def train_epoch(model, train_loader, optimizer, epoch):\n",
        "    model.train()  # Set the nn.Module to train mode. \n",
        "    total_loss = 0\n",
        "    correct = 0\n",
        "    num_samples = len(train_loader.dataset)\n",
        "    for batch_idx, (data, target) in tqdm(enumerate(train_loader), total=len(train_loader)):  # 1) get batch\n",
        "        # Move to cuda\n",
        "        if next(model.parameters()).is_cuda:\n",
        "            data, target = data.cuda(), target.cuda()\n",
        "      \n",
        "        # Reset gradient data to 0\n",
        "        optimizer.zero_grad()\n",
        "        \n",
        "        # Get prediction for batch\n",
        "        output = model(data)\n",
        "        \n",
        "        # 2) Compute loss\n",
        "        loss = F.cross_entropy(output, target)\n",
        "        \n",
        "        # 3) Do backprop\n",
        "        loss.backward()\n",
        "        \n",
        "        # 4) Update model\n",
        "        optimizer.step()\n",
        "        \n",
        "        # Do book-keeping to track accuracy and avg loss\n",
        "        pred = output.max(1, keepdim=True)[1]  # get the index of the max log-probability\n",
        "        correct += pred.eq(target.view_as(pred)).sum().item()\n",
        "        total_loss += loss.detach()  # Don't keep computation graph \n",
        "\n",
        "    print(f'Train Epoch: {epoch} '\n",
        "          f'Loss: {total_loss / num_samples:.4f}, '\n",
        "          f'Accuracy: {correct}/{num_samples} ({100. * correct / num_samples:.0f}%)')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xGZnlI58dcjN",
        "colab_type": "text"
      },
      "source": [
        "## Evaluation Procedure\n",
        "\n",
        "### Review Question:\n",
        "1. What should we change from our training procedure to make the evaluation procedure?"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GPzqSuY3dcjO",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def eval_epoch(model, test_loader, name):\n",
        "    model.eval()\n",
        "    test_loss = 0\n",
        "    correct = 0\n",
        "    for data, target in test_loader:\n",
        "        # Move to cuda\n",
        "        if next(model.parameters()).is_cuda:\n",
        "            data, target = data.cuda(), target.cuda()\n",
        "        \n",
        "        output = model(data)\n",
        "        \n",
        "        test_loss += F.cross_entropy(output, target).item()  # sum up batch loss\n",
        "        pred = output.max(1, keepdim=True)[1]  # get the index of the max log-probability\n",
        "        correct += pred.eq(target.view_as(pred)).sum().item()\n",
        "\n",
        "    test_loss /= len(test_loader.dataset)\n",
        "    print(f'\\n{name} set: '\n",
        "          f'Average loss: {test_loss:.4f}, '\n",
        "          f'Accuracy: {correct}/{len(test_loader.dataset)} ({100. * correct / len(test_loader.dataset):.0f}%)\\n')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NGQFSGYPpU2t",
        "colab_type": "text"
      },
      "source": [
        "# Introduction to Word Embeddings"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "e_PKO6JWgTlZ",
        "colab_type": "text"
      },
      "source": [
        "## Limitation of Bag-of-Words\n",
        "\n",
        "The bag-of-words featurization used in the previous tutorials and exercises (shown below) indicates the presence or absence of individual words or n-grams, but it doesn't encode sequential information.\n",
        "\n",
        "<img src=\"https://cdn-images-1.medium.com/max/1200/1*eUedufAl7_sI_QWSEIstZg.png\">\n",
        "\n",
        "For example, if we are using a bag-of-words with unigrams, then the sentence \"the beer is bad, not good\" and the sentence \"the beer is good, not bad\" have the same featurization because they have the same words, even though they have very different meanings."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DN_ToFEkjpiT",
        "colab_type": "text"
      },
      "source": [
        "## Sequential Featurization\n",
        "\n",
        "A better approach would be to construct a featurization that preserves the order of the sequence.\n",
        "\n",
        "For instance, instead of representing a sentence as a single vector, we could represent a sentence as a sequence of vectors, one for each word.\n",
        "\n",
        "One option is to represent eaach vector as a *one-hot* encoding of the word (i.e. a vector with 1 for the word and 0 for all other words), like below.\n",
        "\n",
        "<img src=\"https://www.tensorflow.org/images/feature_columns/categorical_column_with_vocabulary.jpg\">"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tFkRfCqiROvR",
        "colab_type": "text"
      },
      "source": [
        "## Word Embeddings\n",
        "\n",
        "Although the one-hot featurization described above can work, it is still limited because it provides no information about how words are related to each other.\n",
        "\n",
        "For instance, the words \"play\", \"playing\", and \"igloo\" would have one-hot features `[1,0,0]`, `[0,1,0]`, and `[0,0,1]`, which are all equally different, even though \"play\" and \"playing\" should intuitively have very similar representations.\n",
        "\n",
        "A commonly used alternative to one-hot vectors is *word embeddings*. The idea is to encode a word using a vector of real numbers, i.e. a word embedding, that is *learned* during training.\n",
        "\n",
        "The advantage of using word embeddings is that similar words can end up learning similar embeddings (as seen below), which allows the model to better encode the meaning of sentences.\n",
        "\n",
        "<img src=\"https://shanelynnwebsite-mid9n9g1q9y8tt.netdna-ssl.com/wp-content/uploads/2018/01/word-vector-space-similar-words.png\">"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WCz4MBHKmU_e",
        "colab_type": "text"
      },
      "source": [
        "## Word Embeddings in PyTorch\n",
        "\n",
        "PyTorch makes it easy to use word embeddings with the `nn.Embedding` class. An `nn.Embedding` maps from indices (representing a words) to word embeddings."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Sz9pRTU6mzEC",
        "colab_type": "text"
      },
      "source": [
        "To use a PyTorch `nn.Embedding`, we first need to determine the vocabulary of all words that will be used."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fOLAuhxWyBwm",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "sentence = 'the beer is really really good'\n",
        "vocab = {word for word in sentence.split()}\n",
        "print(f'Vocab = {vocab}')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "V1h-7-6cm7QQ",
        "colab_type": "text"
      },
      "source": [
        "Next, we assign an index to each word in the vocabulary."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "R3haWWMrm_sG",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "word_to_index = {word: index for index, word in enumerate(vocab)}\n",
        "print(f'Word to index = {word_to_index}')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tnusAGznnHcQ",
        "colab_type": "text"
      },
      "source": [
        "Now we can create an `nn.Embedding`."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AAJPUnVPnKO0",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "vocab_size = len(vocab)\n",
        "embed_size = 5  # the dimensionality of the word embeddings\n",
        "embed = nn.Embedding(vocab_size, embed_size)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VFw0MrMnnVRi",
        "colab_type": "text"
      },
      "source": [
        "We can then use the embedding to embed each word in a sentence."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BhwJSTu-nZSd",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "indices = [word_to_index[word] for word in sentence.split()]\n",
        "indices = torch.LongTensor(indices)  # nn.Embedding only works with PyTorch LongTensors\n",
        "embedding = embed(indices)\n",
        "\n",
        "print(f'Sentence = {sentence}')\n",
        "print(f'Indices = {indices}')\n",
        "print(f'Embedding = \\n{embedding}')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "huJ84xJWoQzx",
        "colab_type": "text"
      },
      "source": [
        "## Padding\n",
        "\n",
        "PyTorch tensors and models are required to have fixed dimensions. However, sentences naturally have different lengths since they have different numbers of words. To address this issue, we add *padding*.\n",
        "\n",
        "The idea of padding is to add 0s to shorter sequences so that all sequences are the same length.\n",
        "\n",
        "PyTorch makes padding easy by introducing a `padding_idx` parameter to `nn.Embedding`. Any time the `nn.Embedding` encounters the `padding_idx` in a sequence, it replaces it with a vector of all 0s. That way, the padding has no effect on the model's prediction."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XQDFr9jjpzHb",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Sentences\n",
        "sentence_1 = 'the beer is really really good'\n",
        "sentence_2 = 'the beer sucks'\n",
        "\n",
        "# Get vocab\n",
        "vocab = {word for word in sentence_1.split()} | {word for word in sentence_2.split()}\n",
        "print(f'Vocab = {vocab}')\n",
        "\n",
        "# Get word to index mapping (need +1 b/c padding_idx = 0)\n",
        "word_to_index = {word: index + 1 for index, word in enumerate(vocab)}\n",
        "print(f'Word to index = {word_to_index}')\n",
        "\n",
        "# Map words to indices\n",
        "indices_1 = [word_to_index[word] for word in sentence_1.split()]\n",
        "indices_2 = [word_to_index[word] for word in sentence_2.split()]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0B_4rpPpqlEi",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Add padding\n",
        "padding_idx = 0\n",
        "\n",
        "indices_1 = indices_1 + [padding_idx] * max(0, len(indices_2) - len(indices_1))\n",
        "indices_2 = indices_2 + [padding_idx] * max(0, len(indices_1) - len(indices_2))\n",
        "\n",
        "indices_1 = torch.LongTensor(indices_1)\n",
        "indices_2 = torch.LongTensor(indices_2)\n",
        "\n",
        "print(f'Indices 1 = {indices_1}')\n",
        "print(f'Indices 2 = {indices_2}')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "G2SgR-TStZKb",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Create embedding with padding\n",
        "vocab_size = len(vocab) + 1  # +1 because of padding\n",
        "embed_size = 5  # the dimensionality of the word embeddings\n",
        "embed = nn.Embedding(vocab_size, embed_size, padding_idx=padding_idx)\n",
        "\n",
        "# Embed\n",
        "embedding_1 = embed(indices_1)\n",
        "embedding_2 = embed(indices_2)\n",
        "\n",
        "print(f'Sentence 1 = {sentence_1}')\n",
        "print(f'Indices 1 = {indices_1}')\n",
        "print(f'Embedding 1 = \\n{embedding_1}')\n",
        "print()\n",
        "print(f'Sentence 2 = {sentence_2}')\n",
        "print(f'Indices 2 = {indices_2}')\n",
        "print(f'Embedding 2 = \\n{embedding_2}')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6juQj-48uY5-",
        "colab_type": "text"
      },
      "source": [
        "# Word Embeddings for Beer Reviews"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-Gpe-xALvQKP",
        "colab_type": "text"
      },
      "source": [
        "Now we'll perform the same steps to prepare the beer review dataset for use with word embeddings. The `nn.Embedding` itself will be defined in the model as it is learned along with the other parameters."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jUt7IN02RJvc",
        "colab_type": "text"
      },
      "source": [
        "## Define Vocab and Word-to-Index Mapping"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BT93tF8iL1hr",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Define vocab\n",
        "vocab = {word for text in allText for word in text.split()}\n",
        "\n",
        "# Create word to index mapping\n",
        "padding_idx = 0\n",
        "word_to_index = {word: index + 1 for index, word in enumerate(vocab)}\n",
        "vocab_size = len(word_to_index) + 1\n",
        "\n",
        "print(f'Vocab size = {vocab_size:,}')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oDCNl7nKRHjt",
        "colab_type": "text"
      },
      "source": [
        "## Map Words to Indices"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TEgwodfayJfn",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "trainX = [[word_to_index[word] for word in text.split()] for text in trainText]\n",
        "devX =   [[word_to_index[word] for word in text.split()] for text in devText]\n",
        "testX =  [[word_to_index[word] for word in text.split()] for text in testText]\n",
        "\n",
        "print(f'Indices of first train sentence = {trainX[0]}')\n",
        "print(f'Last five indices = {trainX[0][-5:]}')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "i_OPFDOzQ8eX",
        "colab_type": "text"
      },
      "source": [
        "## Add Padding"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vMbCHXPavmyI",
        "colab_type": "text"
      },
      "source": [
        "Note: Since some beer reviews are extremely long, we've hard coded a maximum sentence length `max_len` in the Model and Training Settings section above."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "S1K4Hg5DyimQ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "trainX = [seq[:max_len] + [padding_idx] * (max_len - len(seq)) for seq in trainX]\n",
        "devX =   [seq[:max_len] + [padding_idx] * (max_len - len(seq)) for seq in devX]\n",
        "testX =  [seq[:max_len] + [padding_idx] * (max_len - len(seq)) for seq in testX]\n",
        "\n",
        "print(f'Indices of first train sentence = {trainX[0]}')\n",
        "print(f'Last five indices = {trainX[0][-5:]}')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hZLu0JzjQ2LF",
        "colab_type": "text"
      },
      "source": [
        "## Build Dataset/DataLoader"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2qkAAPU60onl",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Build Dataset\n",
        "train = BeerReviewDataset(trainX, trainY)\n",
        "dev = BeerReviewDataset(devX, devY)\n",
        "test = BeerReviewDataset(testX, testY)\n",
        "\n",
        "# Build DataLoader\n",
        "train_loader = DataLoader(train, batch_size=batch_size, shuffle=True)\n",
        "dev_loader = DataLoader(dev, batch_size=batch_size, shuffle=True)\n",
        "test_loader = DataLoader(test, batch_size=batch_size, shuffle=True)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "P0lq22LOcj0j",
        "colab_type": "text"
      },
      "source": [
        "### Review Question\n",
        "1. Why is important to shuffle the data?\n",
        "2. Is it important to shuffle the test data?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZfXkqNFyqBsI",
        "colab_type": "text"
      },
      "source": [
        "# Multi-Layer Perceptron (MLP)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qb2uLyKZwhQK",
        "colab_type": "text"
      },
      "source": [
        "Before experimenting with RNNs and CNNs, we'll review the basic feed-forward neural network (aka the multi-layer perceptron, or MLP), seen below.\n",
        "\n",
        "<img src=\"https://freecontent.manning.com/wp-content/uploads/Teofili_WDDLCtS_02.png\">\n",
        "\n",
        "In the previous tutorials and labs, we build an MLP that operated on bag-of-words features. Below, we adapt the MLP to instead use word embeddings.\n",
        "\n",
        "However, since an MLP is not a sequence model like an RNN or CNN,  it can't handle the extra sequential dimension we just introduced. To handle this, we simply remove that dimension by summing the embeddings, thereby providing the MLP with a since vector for the whole sentence."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JNQEIUKGqLM4",
        "colab_type": "text"
      },
      "source": [
        "## Define MLP\n",
        "\n",
        "### Review Question:\n",
        "\n",
        "1. Why is this more powerful than a single linear layer?\n",
        "2. How can we make an MLP more powerful?\n",
        "3. What are the limitations of an MLP?"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rOD5dOZEqBAD",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class MLP(nn.Module):\n",
        "    def __init__(self, vocab_size, padding_idx, embedding_size, hidden_size, output_size, dropout):\n",
        "        super(MLP, self).__init__()\n",
        "        \n",
        "        # Embedding layer\n",
        "        self.embed = nn.Embedding(vocab_size, embedding_size, padding_idx=padding_idx)\n",
        "        \n",
        "        # Fully connected layers\n",
        "        self.fc1 = nn.Linear(embedding_size, hidden_size)\n",
        "        self.fc2 = nn.Linear(hidden_size, hidden_size)\n",
        "        self.fc3 = nn.Linear(hidden_size, output_size)\n",
        "        \n",
        "        # Dropout (regularization)\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "\n",
        "    def forward(self, x):  # batch_size x seq_length\n",
        "        # Embed\n",
        "        embedded = self.embed(x)  # batch_size x seq_length x embedding_size\n",
        "        \n",
        "        # Sum embeddings\n",
        "        embedded = embedded.sum(dim=1)  # batch_size x embedding_size\n",
        "        \n",
        "        # MLP\n",
        "        hidden = F.relu(self.fc1(embedded))  # batch_size x hidden_size\n",
        "        hidden = self.dropout(F.relu(self.fc2(hidden)))  # batch_size x hidden_size\n",
        "        logit = self.fc3(hidden)  # batch_size x output_size\n",
        "        \n",
        "        return logit"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IDTl_YiUqSMK",
        "colab_type": "text"
      },
      "source": [
        "## Build MLP"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "osUGOQsvqUzB",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "model = MLP(vocab_size, padding_idx, embedding_size, hidden_size, output_size, dropout)\n",
        "\n",
        "print(model)\n",
        "print(f'Number of parameters = {param_count(model):,}')\n",
        "\n",
        "# Move to cuda\n",
        "if use_cuda and torch.cuda.is_available():\n",
        "    model = model.cuda()\n",
        "\n",
        "optimizer = optim.Adam(model.parameters(), lr=lr, weight_decay=weight_decay) "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zCdqhdnVqV5k",
        "colab_type": "text"
      },
      "source": [
        "## Train MLP"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oXt9Oxz-qWzn",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "for epoch in range(1, epochs + 1):\n",
        "    train_epoch(model, train_loader, optimizer, epoch)\n",
        "    eval_epoch(model,  dev_loader, \"Dev\")\n",
        "    print(\"---\")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xE7LcxEPqW69",
        "colab_type": "text"
      },
      "source": [
        "## Test MLP"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gRTfwoZHqX5k",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "eval_epoch(model,  test_loader, \"Test\")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZnsfujvesrvU",
        "colab_type": "text"
      },
      "source": [
        "# Recurrent Neural Network (RNN)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rrm8CHPmxt-0",
        "colab_type": "text"
      },
      "source": [
        "Unlike MLPs, recurrent neural networks (RNNs) can handle sequential data. RNNs work by processing a sequence one token at a time while maintaining a sense of state that is updated by each token in the sequence. This allows the RNN to incorporate information from all tokens in the sequence *in the order* in which they appear. This sequential information is what makes RNNs (and CNNs) more powerful than the MLP.\n",
        "\n",
        "In the code below, we use one particular kind of RNN called a Long Short-Term Memory (LSTM) encoder.\n",
        "\n",
        "<img src=\"https://i.ytimg.com/vi/kMLl-TKaEnc/maxresdefault.jpg\">"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zY_yh_IPA3hK",
        "colab_type": "text"
      },
      "source": [
        "## Define RNN"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WdLyh493swKI",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class RNN(nn.Module):\n",
        "    def __init__(self, vocab_size, padding_idx, embedding_size, hidden_size, output_size, dropout):\n",
        "        super(RNN, self).__init__()\n",
        "        \n",
        "        # Embedding layer\n",
        "        self.embed = nn.Embedding(vocab_size, embedding_size, padding_idx=padding_idx)\n",
        "        \n",
        "        # LSTM (RNN)\n",
        "        self.rnn = nn.LSTM(\n",
        "            input_size=embedding_size,\n",
        "            hidden_size=hidden_size,\n",
        "            batch_first=True\n",
        "        )\n",
        "        \n",
        "        # Fully connected layer\n",
        "        self.output = nn.Linear(hidden_size, output_size)\n",
        "        \n",
        "        # Dropout (regularization)\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "        \n",
        "    def forward(self, x):  # batch_size x seq_length\n",
        "        # Embed\n",
        "        embedded = self.embed(x)  # batch_size x seq_length x embedding_size\n",
        "      \n",
        "        # Run RNN\n",
        "        o, _ = self.rnn(embedded)  # batch_size x seq_length x hidden_size\n",
        "        \n",
        "        # Dropout\n",
        "        o = self.dropout(o)  # batch_size x seq_length x hidden_size\n",
        "        \n",
        "        # Max pooling across sequence\n",
        "        o, _ = torch.max(o, dim=1)    # batch_size x hidden_size\n",
        "        \n",
        "        # Output layer\n",
        "        logit = self.output(o)  # batch_size x output_size\n",
        "        \n",
        "        return logit"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OWpZQ_BihzRi",
        "colab_type": "text"
      },
      "source": [
        "### Review Question:\n",
        "1. In this implementation, we take the max across all output states. What are some other options?\n",
        "2. How should the speed of an RNN compared to the MLP? Will this have move parameters?\n",
        "3. Do we need to change anything in our train or eval loop?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QOxa8rZCA446",
        "colab_type": "text"
      },
      "source": [
        "## Build RNN"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oTw0eOMW0nR1",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "model = RNN(vocab_size, padding_idx, embedding_size, hidden_size, output_size, dropout)\n",
        "\n",
        "print(model)\n",
        "print(f'Number of parameters = {param_count(model):,}')\n",
        "\n",
        "# Move to cuda\n",
        "if use_cuda and torch.cuda.is_available():\n",
        "    model = model.cuda()\n",
        "    \n",
        "optimizer = optim.Adam(model.parameters(), lr=lr, weight_decay=weight_decay) "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gaDSVZMXA7zO",
        "colab_type": "text"
      },
      "source": [
        "## Train RNN"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DUR9k6471fFI",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "for epoch in range(1, epochs + 1):\n",
        "    train_epoch(model, train_loader, optimizer, epoch)\n",
        "    eval_epoch(model,  dev_loader, \"Dev\")\n",
        "    print(\"---\")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vAOPc283A9t2",
        "colab_type": "text"
      },
      "source": [
        "## Test RNN"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tvS5w0e91fbH",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "eval_epoch(model,  test_loader, \"Test\")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9sOI-H2e7sUi",
        "colab_type": "text"
      },
      "source": [
        "# Convolutional Neural Network (CNN)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BGadBrFb0rzr",
        "colab_type": "text"
      },
      "source": [
        "Like RNNs, convolutional neural networks (CNNs) can also handle sequential data. However, rather than processing input tokens one-by-one, a CNN performs operations on several neighboring tokens simultaneously, similar to the n-gram approach (but with access to the full sequence).\n",
        "\n",
        "For text, we use 1-dimensional convolutions, as illustrated below. The same convolutional weights `w1`, `w2`, and `w3` are used repeatedly throughout the sequence, allowing the model to filter for certain words or phrases in the sequence.\n",
        "\n",
        "<img src=\"https://qph.fs.quoracdn.net/main-qimg-523434af0d21bb0b59454aa9563cc90b.webp\">\n",
        "\n",
        "CNNs can also be applied to images (2D convolutions) or videos (3D convolutions).\n",
        "\n",
        "<img src=\"https://s3.amazonaws.com/cdn.ayasdi.com/wp-content/uploads/2018/06/21100605/Fig2GCNN1.png\">"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ubU9eP8bBBFb",
        "colab_type": "text"
      },
      "source": [
        "## Define CNN"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "klUA9Ugi7u5z",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class CNN(nn.Module):\n",
        "    def __init__(self, vocab_size, padding_idx, embedding_size, hidden_size, output_size, dropout):\n",
        "        super(CNN, self).__init__()\n",
        "        \n",
        "        # Embedding layer\n",
        "        self.embed = nn.Embedding(vocab_size, embedding_size, padding_idx=padding_idx)\n",
        "        \n",
        "        # Convolutional layers\n",
        "        self.conv1 = nn.Conv1d(in_channels=embedding_size, out_channels=hidden_size, kernel_size=3, padding=0)\n",
        "        self.conv2 = nn.Conv1d(in_channels=embedding_size, out_channels=hidden_size, kernel_size=5, padding=1)\n",
        "        self.conv3 = nn.Conv1d(in_channels=embedding_size, out_channels=hidden_size, kernel_size=7, padding=2)\n",
        "        \n",
        "        # Fully connect layer\n",
        "        self.output = nn.Linear(hidden_size, output_size)\n",
        "        \n",
        "        # Dropout (regularization)\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "        \n",
        "    def forward(self, x):  # batch_size x seq_length\n",
        "        # Embed\n",
        "        embedded = self.embed(x)  # batch_size x seq_length x embedding_size\n",
        "      \n",
        "        # Permute dimensions\n",
        "        embedded = embedded.permute(0, 2, 1)  # batch_size x embedding_size x seq_length\n",
        "        \n",
        "        # Convolutional layers\n",
        "        hidden_1 = self.dropout(F.relu(self.conv1(embedded)))  # batch_size x hidden_size x new_seq_length\n",
        "        hidden_2 = self.dropout(F.relu(self.conv2(embedded)))  # batch_size x hidden_size x new_seq_length\n",
        "        hidden_3 = self.dropout(F.relu(self.conv3(embedded)))  # batch_size x hidden_size x new_seq_length\n",
        "        \n",
        "        # Sum\n",
        "        hidden = hidden_1 + hidden_2 + hidden_3    # batch_size x hidden_size x new_seq_length\n",
        "        \n",
        "        # Max pooling across sequence\n",
        "        hidden, _ = hidden.max(dim=-1)    # batch_size x hidden_size\n",
        "        \n",
        "        # Output\n",
        "        logit = self.output(hidden)  # batch_size x output_size\n",
        "        \n",
        "        return logit"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XuaRaqdtBCrM",
        "colab_type": "text"
      },
      "source": [
        "## Build CNN"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZD95lEXmAmGQ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "model = CNN(vocab_size, padding_idx, embedding_size, hidden_size, output_size, dropout)\n",
        "\n",
        "print(model)\n",
        "print(f'Number of parameters = {param_count(model):,}')\n",
        "\n",
        "if use_cuda and torch.cuda.is_available():\n",
        "    model = model.cuda()\n",
        "\n",
        "optimizer = optim.Adam(model.parameters(), lr=lr, weight_decay=weight_decay) "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sWwArpwcBEeo",
        "colab_type": "text"
      },
      "source": [
        "## Train CNN"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eNN7Zrr2Ap9D",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "for epoch in range(1, epochs + 1):\n",
        "    train_epoch(model, train_loader, optimizer, epoch)\n",
        "    eval_epoch(model,  dev_loader, \"Dev\")\n",
        "    print(\"---\")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3wROoo34BGBj",
        "colab_type": "text"
      },
      "source": [
        "## Test CNN"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KxZmibBj-GDK",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "eval_epoch(model,  test_loader, \"Test\")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3yT2AdGfiRwG",
        "colab_type": "text"
      },
      "source": [
        "### Discussion Questions:\n",
        "1. In are the advantages and disadvantages of RNN vs CNN?\n",
        "2. How do you get a CNN to take in a wider context?\n",
        "3. Why isn't the accuracy higher?"
      ]
    }
  ]
}